{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejask108/Yes-Bank-Stock-Closing-Price-Prediction/blob/main/Yes_Bank_Stock_Closing_Price_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Yes Bank Stock Closing Price Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Team\n",
        "##### **Team Member 1 -** Rakshanda Shaikh\n",
        "##### **Team Member 2 -**Nirvi Prabhale\n",
        "##### **Team Member 3 -**Tejesh Khadke\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words.\n",
        "\n",
        "This project focuses on a critical challenge: predicting the closing price of Yes Bank's stock. This prediction is crucial for stakeholders, investors, and market participants due to the bank's recent difficulties, like bad loans and fraud cases. Regulatory intervention by the Reserve Bank of India has added complexity to predicting the bank's stock prices.\n",
        "\n",
        "To address this, the project uses a comprehensive dataset covering Yes Bank's stock prices since its inception. This dataset includes key metrics like monthly closing, starting, highest, and lowest prices. The goal is to create predictive models that can grasp the complex trends in the bank's stock prices, considering its turbulent history.\n",
        "\n",
        "The project employs various modeling techniques, including time series models and regression methods. These techniques aim to accurately forecast the closing price of Yes Bank's stock and account for significant events like fraud cases and regulatory interventions.\n",
        "\n",
        "By achieving accurate predictions, this project provides valuable insights for stakeholders' investment decisions. It navigates the complexities and uncertainties surrounding Yes Bank's stock prices, ultimately aiding in better decision-making and understanding the bank's financial performance."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Rakshanda19/Yes-Bank-Stock-Closing-Price-Prediction.git"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project centers on developing a robust predictive model for forecasting the closing price of Yes Bank's stock. The primary challenge involves comprehending the intricate dynamics of stock price trends, particularly the abrupt decline post-2018 after a period of growth. A significant hurdle lies in managing multicollinearity within the dataset, arising from interrelated independent variables. Tackling this issue is crucial to ensure the model's accuracy in prediction by adequately considering each variable's contribution.\n",
        "\n",
        "In addition, the model should be adept at accounting for major events that have left an impact on Yes Bank's stock performance. Notable occurrences, such as the involvement of the bank's founders in fraud cases and regulatory interventions by the Reserve Bank of India, have the potential to sway stock prices significantly. A key objective is for the predictive model to adeptly capture and reflect the influence of such events with precision.\n",
        "\n",
        "Furthermore, the project sets out to attain a level of predictive accuracy akin to the benchmark set by the K-Nearest Neighbors (KNN) Regression model, which achieved an impressive 99% accuracy. This high accuracy level is crucial for providing valuable insights to stakeholders, investors, and market participants. Armed with a reliable predictive tool, they can make well-informed decisions regarding investments in Yes Bank's stock, even amidst the intricate challenges posed by its price fluctuations. Ultimately, this endeavor strives to empower stakeholders with a dependable means of navigating the complexities of Yes Bank's stock performance landscape.\n",
        "\n"
      ],
      "metadata": {
        "id": "CKne7_MX6RPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Import necessary libraries\n",
        "\n",
        "# Import NumPy for numerical computations\n",
        "import numpy as np\n",
        "\n",
        "# Import Pandas for data manipulation and analysis\n",
        "import pandas as pd\n",
        "\n",
        "# Import Matplotlib for basic data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import Plotly express for interactive visualisations\n",
        "import plotly.express as px\n",
        "\n",
        "# Import Seaborn for advanced statistical visualizations\n",
        "import seaborn as sns\n",
        "\n",
        "# Import Plotly graph objects for interactive visualizations\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Import the datetime module for working with dates and times\n",
        "from datetime import datetime\n",
        "\n",
        "# Import warnings module to ignore potential warnings\n",
        "import warnings\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "#Nirvi\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "stock_df=pd.read_csv(\"/content/drive/MyDrive/data_YesBank_StockPrices.csv\")\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rakshanda\n",
        "stock_df=pd.read_csv('/content/data_YesBank_StockPrices (1).csv')"
      ],
      "metadata": {
        "id": "yKmO81Yf7his"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "stock_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_df.tail()"
      ],
      "metadata": {
        "id": "wv_Acph98DLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "stock_df.shape\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shape of the dataframe is : (185,5)"
      ],
      "metadata": {
        "id": "aoh0n5Uk8jPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "stock_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above analysis, we can conclude that :\n",
        "            \n",
        "*  The shape of our dataset is 185 rows and 5 columns.\n",
        "*  Datatype of Date is given as object which we need to change that to DateTime. The Date column contains the date of the stock price. The data type of this column is currently object, which means that the values in this column are strings. We need to change the data type of this column to DateTime so that we can perform date-related operations on it, such as calculating the day of the week, the month, or the year.\n",
        "*  Rest all features have float value as data point. The other 4 columns in the dataset contain floating-point numbers. These numbers represent the open price, high price, low price, and close price of the stock. The volume column contains the number of shares traded on a given day.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QtorZDkn9dHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "stock_df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of duplicate rows is : 0"
      ],
      "metadata": {
        "id": "4IWvwJHGDYbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "stock_df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import missingno as msno\n",
        "\n",
        "msno.matrix(stock_df)\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above analysis, we can conclude that :\n",
        "            \n",
        "*  The shape of our dataset is 185 rows and 5 columns.\n",
        "*  Datatype of Date is given as object which we need to change that to DateTime. The Date column contains the date of the stock price. The data type of this column is currently object, which means that the values in this column are strings. We need to change the data type of this column to DateTime so that we can perform date-related operations on it, such as calculating the day of the week, the month, or the year.\n",
        "*  Rest all features have float value as data point. The other 4 columns in the dataset contain floating-point numbers. These numbers represent the open price, high price, low price, and close price of the stock. The volume column contains the number of shares traded on a given day.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "stock_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "stock_df.describe(include=\"all\")"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset consists of monthly observations of Yes Bank stock prices since its listing on the stock exchange. The dataset includes the following features:\n",
        "\n",
        "Date: This indicates the specific month for which the stock price is recorded.\n",
        "\n",
        "Open: This represents the price of the stock at the beginning of the trading day when the stock exchange opens.\n",
        "\n",
        "High: This indicates the highest price reached by the stock during the given month.\n",
        "\n",
        "Low: This indicates the lowest price reached by the stock during the given month\n",
        "\n",
        "Close: This represents the price of the stock at the end of the trading day when the stock exchange closes.\n",
        "\n",
        "The dataset provides a comprehensive overview of the monthly performance of Yes Bank stock, including the opening, highest, lowest, and closing prices for each month since its listing on the stock exchange.\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "\n",
        "for variable in stock_df.columns:\n",
        "  print(f\"The unique values for the '{variable}' variable are:\\n\\n {stock_df[variable].unique()}\\n\\n\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "# Let us now save original data before making any chnages in it.\n",
        "# Saving a copy of the original dataframe\n",
        "stock1_df = stock_df.copy()\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data type Correction**\n",
        "\n",
        "The dataset is clean, with no duplicate or missing values, so there's no need for data adjustments. Moving on, we are addressing outliers in the data.\n",
        "\n",
        "However, the Date column's datatype is currently listed as 'object'. To accurately represent date and time, we have converted it to the datetime format. The 'object' type isn't suitable for this kind of data.\n",
        "\n",
        "To make this change, we have used the pd.to_datetime() function. For instance, this code would transform the Date column's datatype to datetime:"
      ],
      "metadata": {
        "id": "UaPdQ5TNFtIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert string object to datetime object\n",
        "stock_df['Date'] = stock_df['Date'].apply(lambda x: datetime.strptime(x, \"%b-%y\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "fdWLIVF6E4vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stock_df)"
      ],
      "metadata": {
        "id": "8bE4lEdIFE77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting Date column as index.\n",
        "stock_df.set_index('Date',inplace=True)\n",
        "stock_df.head()"
      ],
      "metadata": {
        "id": "FcSwfcI2GCQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#categorical columns\n",
        "cat_columns=stock_df.select_dtypes(include='object').columns\n",
        "print(f'categorical columns:{list(cat_columns)}')\n"
      ],
      "metadata": {
        "id": "fPDc9493HR4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#non-categorical columns\n",
        "num_columns=stock_df.select_dtypes(exclude='object').columns\n",
        "print('non-categorical columns:',list(num_columns))"
      ],
      "metadata": {
        "id": "b4VY1X6DHR1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dependent_variable = ['Close']\n",
        "independent_variables = list(stock_df.columns[:-1])"
      ],
      "metadata": {
        "id": "LYSCeWNqDJ1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig = plt.figure(figsize =(10, 7))\n",
        "boxplot = stock_df.boxplot(column=['Open','High','Low',\"Close\"],grid=False,notch=True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "owFqHxdALcrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As above boxplot shows outliers this is because of stock price fall from nearly around 400 to 20.This happen quick within very few months thats why top value of stocks looks like outliers."
      ],
      "metadata": {
        "id": "p6asZvNKM8Kf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset doesn't have any null value and duplicate value.\n",
        "We did copy of our data to preserve original data also converted to date variable from object datatype to date datatype.\n",
        "Set date columns as index to track variation in stock price\n",
        ".Upon examining the provided dataframe, it becomes apparent that all the columns exclusively consist of numerical data.\n",
        "\n",
        "\n",
        "Furthermore, during the examination of the dataset, it is evident that outliers are present. These outliers are data points that significantly deviate from the majority of the data. Before proceeding with modeling or conducting further analysis, it is crucial to address these outliers. Dealing with outliers involves assessing their impact on the data and making decisions regarding appropriate actions, such as removing or transforming them. By addressing the outliers, we can enhance the robustness and reliability of our models and analyses."
      ],
      "metadata": {
        "id": "HtcobtKDOAEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Univariate Analysis**"
      ],
      "metadata": {
        "id": "SssTTYUaemZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 - **Candle stick graph with price movement**"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Figure object with Candlestick chart\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(go.Candlestick(\n",
        "    x = stock_df.index,            # x-axis values (dates)\n",
        "    open = stock_df['Open'],       # open prices\n",
        "    high = stock_df['High'],       # high prices\n",
        "    low = stock_df['Low'],         # low prices\n",
        "    close = stock_df['Close']      # close prices\n",
        "))\n",
        "\n",
        "# Update the layout of the figure with a title\n",
        "fig.update_layout(\n",
        "    title={'text': 'Describing the Price Movements', 'x': 0.5, 'y': 0.95, 'font': {'color': 'white'}},\n",
        "    xaxis=dict(title='Year', title_font={'color': 'white'}, tickfont={'color': 'white'}),\n",
        "    yaxis=dict(title='Price', title_font={'color': 'white'}, tickfont={'color': 'white'}),\n",
        "    width=800,\n",
        "    height=800,\n",
        "    plot_bgcolor='rgb(36, 40, 47)',  # Set the background color to a professional dark gray\n",
        "    paper_bgcolor='rgb(51, 56, 66)'  # Set the paper color\n",
        ")\n",
        "\n",
        "\n",
        "# Show the figure\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Candlestick chart was chosen due to its effectiveness in displaying crucial price information. It represents open, high, low, and close prices, making it valuable for financial analysis. This chart is particularly useful for stocks, conveying market sentiment and trends. Each candlestick covers a time interval and its color and shape indicate price changes. High and low points show peak prices, while the body displays opening and closing prices. This helps identify patterns, trends, and potential reversals, aiding informed decisions on buying or selling assets. The larger size enhances visibility, allowing detailed analysis. Overall, the Candlestick chart is a powerful tool for understanding price dynamics in financial markets."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analysis of Yes Bank's stock prices reveals a clear pattern. Before 2018, the stock consistently grew, showing investor optimism. However, a sharp drop followed, mainly due to the fraud case involving former CEO Rana Kapoor.\n",
        "\n",
        "Until 2018, the stock consistently rose, reflecting positive market conditions. The fraud case changed everything, causing a steep decline.\n",
        "\n",
        "The Rana Kapoor fraud case damaged investor trust, leading to a significant stock value drop. This event negatively impacted the company's reputation and stability.\n",
        "\n",
        "In summary, the analysis shows two distinct trends: growth before 2018 and a significant post-2018 decline due to the Rana Kapoor fraud case."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights could potentially have a positive business impact. Understanding the influence of the Yes Bank fraud case on the stock prices helps the bank and investors comprehend the consequences of such events. This awareness can guide future decision-making, risk management, and communication strategies to rebuild trust and investor confidence.\n",
        "\n",
        "The insights indeed lead to negative growth. The Yes Bank fraud case caused a sharp decline in stock prices. Increased scrutiny and regulatory interventions created uncertainty about the bank's future, leading investors to sell shares. This negative sentiment directly impacted stock prices and reflects the tangible impact of external events on financial performance."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2- **Distribution of dependent variable Close Price of stock**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "# Set the figure size and title\n",
        "plt.figure(figsize=(15, 9))\n",
        "plt.suptitle('Overall Distribution of Each Variable', color='white')\n",
        "\n",
        "# Define the color list for each variable (using Yes Bank color scheme)\n",
        "color_list = ['#003366', '#FF6600', '#99CC00', '#FFCC00']\n",
        "\n",
        "\n",
        "for i, column in enumerate(stock_df.columns):\n",
        "    # Create subplots\n",
        "    ax1 = plt.subplot(2, 2, i + 1)\n",
        "    ax2 = ax1.twinx()\n",
        "\n",
        "    # Plot histogram\n",
        "    sns.histplot(stock_df[column], color=color_list[i], ax=ax1)\n",
        "\n",
        "    # Plot KDE curve\n",
        "    sns.kdeplot(stock_df[column], color=color_list[i], ax=ax2)\n",
        "\n",
        "    # Set gridlines\n",
        "    ax1.grid(which='major', alpha=0.5)\n",
        "    ax1.grid(which='minor', alpha=0.5)\n",
        "\n",
        "    # Add vertical lines for mean and median\n",
        "    plt.axvline(stock_df[column].mean(), color='white', linestyle='dashed', linewidth=1.5)\n",
        "    plt.axvline(stock_df[column].median(), color='yellow', linestyle='dashed', linewidth=1.5)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above chart, a mix of histograms and KDE plots, effectively displays how data is distributed in the dataset. It shows center, spread, and shape of distributions, allowing easy comparison. Colors match Yes Bank branding. The chart helps explore skewness, multimodality, and outliers. It's a compact representation, combining histograms' frequency view and KDE's smooth curve. This cohesive chart aids pattern spotting and variable relationships, offering insights into the dataset's nature."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights from the chart show that open, high, low, and close distributions are positively skewed. This means most data is on the left side, with a tail on the right for larger values. Histograms and KDE plots highlight this. Positive skewness suggests these variables tend to have higher values but fewer occurrences. This might indicate restrictions, leading to more lower-end values and some larger ones. Handling this skewness accurately is vital for analysis, possibly needing transformations or different techniques."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights about positively skewed distributions can positively impact decision-making and highlight buying opportunities. However, positive skewness alone doesn't guarantee negative growth. Negative growth depends on multiple factors beyond skewness, like trends and market conditions. Concluding negative growth solely based on skewness isn't justified. Further analysis is needed to understand potential negative impacts on business growth.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 - **Plotting target variable**"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "stock_df['Close'].plot(color='blue')\n",
        "plt.grid(which='major',linestyle='-',linewidth='0.5',color='black')\n",
        "plt.grid(which='minor',linestyle='-',linewidth='0.5',color='black')\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart, a line plot of the 'Close' data, was chosen to visualize the trend and fluctuations in the closing prices of the Yes Bank stock over time.\n",
        "\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight from the chart is that the stock price increased until 2018, followed by a sharp decline after the Rana Kapoor fraud case."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insight about the stock price increasing until 2018 and then sharply declining due to the Rana Kapoor fraud case can be valuable for creating a positive business impact. It allows stakeholders and decision-makers to understand the significant events that affected the stock price and to adjust strategies accordingly.\n",
        "\n",
        "The insight doesn't directly indicate negative growth, but it highlights a specific event (fraud case) that led to a decline. However, negative growth would require a comprehensive analysis considering various factors like market conditions, financial performance, and external influences beyond this specific event.\n"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Bivariate Analysis**"
      ],
      "metadata": {
        "id": "Tj-n7b8HRW3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 - **Distribution of numerical features High, Low and Open price of stock.**"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Chart visualization\n",
        "# List of independent features\n",
        "numerical_features = list(set(stock_df.describe().columns)-{'Close'})\n",
        "numerical_features"
      ],
      "metadata": {
        "id": "lgu1sXpAZGIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Plotting distribution for each of numerical features.\n",
        "for col in numerical_features:\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.distplot(stock_df[col], color='green')\n",
        "    plt.title(\"Distribution\", fontsize=16)\n",
        "    plt.xlabel(col, fontsize=12)\n",
        "    plt.ylabel('Density', fontsize=12)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H87KTGKrZVAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-It looks all numerical features are rightly skewed.\n",
        "\n",
        "-We need to Apply log transformation to make normal."
      ],
      "metadata": {
        "id": "t1EXHAx7aWEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying log transformation\n",
        "for col in numerical_features:\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.distplot(np.log10(stock_df[col]), color='green')\n",
        "    plt.title(\"Distribution\", fontsize=16)\n",
        "    plt.xlabel(col, fontsize=12)\n",
        "    plt.ylabel('Density', fontsize=12)\n",
        "plt.show"
      ],
      "metadata": {
        "id": "Q8tlnaT4acdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used line plot to show how a specific variable changes over time or across a range. For stock prediction, it can illustrate the actual and predicted stock prices over different time periods. This visual helps understand model performance, identify trends, and evaluate prediction accuracy at a glance."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights can include assessing data distributions, identifying outliers, gauging data spread, checking for normality, and comparing feature distributions. These insights help in understanding and preprocessing the data for analysis or modeling."
      ],
      "metadata": {
        "id": "3wDrIRYvNGcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights can potentially lead to a positive business impact by helping with data preprocessing and analysis, which can improve decision-making and model performance. For example, identifying outliers and understanding data distributions can lead to more accurate predictions and better risk management."
      ],
      "metadata": {
        "id": "qUkyR4sjNCgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 **Scatter Plot to see the Best Fit line**"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best Fit Line:- A line of best fit is a straight line that is the best approximation of the given set of data. It is used to study the nature of the relation between two variables."
      ],
      "metadata": {
        "id": "qhLQ3Nv7znJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in numerical_features:\n",
        "   fig = plt.figure(figsize=(9, 8))\n",
        "   ax = fig.gca()\n",
        "   feature = stock_df[col]\n",
        "   label = stock_df['Close']\n",
        "   correlation = feature.corr(label)\n",
        "   plt.scatter(x=feature, y=label,marker=\"^\",c=\"b\",s = label*2)\n",
        "   plt.xlabel(col)\n",
        "   plt.ylabel('Close')\n",
        "   ax.set_title(col + ' Vs. Close' + '         Correlation: ' + str(round(correlation,2)), fontsize=16)\n",
        "   z = np.polyfit(stock_df[col], stock_df['Close'], 1)\n",
        "   y_hat = np.poly1d(z)(stock_df[col])\n",
        "\n",
        "   plt.plot(stock_df[col], y_hat, \"r\", lw=1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oUU6Ivq6zA5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using scatter plots with a best fit line allows for visualizing the relationship between numerical features and the 'Close' price. The correlation coefficient quantifies the strength of the relationship. The best fit line provides an estimate of the trend and predictive power. The plot aids interpretation and communication of the relationship to stakeholders. Annotations, such as the correlation coefficient, provide valuable insights. Customization enhances clarity and aesthetics. The plots help identify potential predictors and support analysis and decision-making in stock market analysis."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After reviewing scatter plots with the best fit line, it's clear that all independent variables have a linear connection with the dependent variable, 'Close'. This means their changes go hand-in-hand predictably.\n",
        "\n",
        "A linear relationship holds significance for analysis and modeling. It implies changes in independent variables relate proportionally to changes in 'Close'. This insight helps build regression models, predict outcomes, and understand the independent variables' influence on 'Close' price.\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying linear relationships between independent variables and the dependent variable has significant business impact, especially in stock market analysis:\n",
        "\n",
        "Prediction and Forecasting: Clear linear relationships allow regression models to predict future 'Close' prices, aiding investment forecasts.\n",
        "\n",
        "Risk Assessment: Analyzing relationships helps assess risk from independent variable changes, supporting risk management.\n",
        "\n",
        "Feature Selection: Recognizing influential variables guides selection for future analyses and model development.\n",
        "\n",
        "Strategy Development: Linear relationships offer insights into stock price drivers, helping develop trading strategies.\n",
        "\n",
        "Understanding these relationships enhances forecasting, risk management, and decision-making for more accurate and informed choices in financial markets.\n"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chart - 6 :- **Skewness in the Dataset**"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chart - 6 visualization\n",
        "\n",
        "**Data Distribution and mean and median of each single Indpendent variable**"
      ],
      "metadata": {
        "id": "BJSKvDUo3bHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = stock_df.describe().columns\n",
        "for col in numeric_features[0:4]:\n",
        "    fig = plt.figure(figsize=(9, 6))\n",
        "    ax = fig.gca()\n",
        "    feature = stock_df[col]\n",
        "    feature.hist(bins=50, ax = ax)\n",
        "    ax.axvline(feature.mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "    ax.axvline(feature.median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "    ax.set_title(col)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TcdCo-5-6BsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Histograms are versatile tools that help researchers, analysts, and decision-makers gain insights into data distributions, patterns, and characteristics. They are a fundamental part of exploratory data analysis and are often the first step in understanding a dataset before applying more advanced statistical techniques.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The charts display the data distribution of each variable in your dataset. The magenta dashed line represents the mean, while the cyan dashed line represents the median. When the mean is close to the peak of the histogram, it suggests a relatively symmetric distribution, but if it's distant from the peak, skewness may be present. The relationship between the mean and median indicates the direction of skewness: a mean to the right of the median suggests positive skew (right-skewed), while a mean to the left suggests negative skew (left-skewed). These visual cues offer valuable insights into the shape and skewness of each variable's data distribution."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, the insights gained from data analysis can be a valuable tool for making informed decisions, optimizing operations, and identifying growth opportunities. However, it's crucial for businesses to interpret these insights accurately and take appropriate actions to leverage them for positive impact and avoid negative consequences. The insights themselves do not inherently cause negative growth but can indirectly contribute to it if not acted upon wisely."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 **Correlation between each independent variable using scatter plot**"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "fig=px.scatter(stock_df, x= 'High', y='Close', title= 'Relations between High and Close')\n",
        "fig.update_layout (autosize=False, width=1000,height=500)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig=px.scatter(stock_df, x= 'Open', y='Close', title= 'Relations between Open and Close')\n",
        "fig.update_layout (autosize=False, width=1000,height=500)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "d9jBKVv1Stvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig=px.scatter(stock_df, x= 'Low', y='Close', title= 'Relations between Low and Close')\n",
        "fig.update_layout (autosize=False, width=1000,height=500)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "DEj2fwfjSwcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart, a scatter plot, was chosen to visualize the relationship between the 'Low' and 'Close' prices of the Yes Bank stock.\n",
        "\n"
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights reveal strong correlations between all independent variables and the dependent variable."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insight about strong correlations between independent variables and the dependent variable can potentially create a positive business impact. It suggests that changes in independent variables are associated with changes in the dependent variable. This understanding can aid in making informed decisions, optimizing strategies, and enhancing predictive models.\n",
        "\n",
        "However, these insights don't directly lead to negative growth. High correlations indicate relationships, but not necessarily causation or direction. Negative growth would require comprehensive analysis considering external factors and trends beyond correlations. So, while correlations offer valuable insights, they don't inherently predict negative growth.\n",
        "\n"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 **The relationship between dependent & independent variables**"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "fig=px.scatter(stock_df, x= 'High', y='Close', title= 'Relations between High and Close')\n",
        "fig.update_layout (autosize=False, width=1000,height=500)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have used scatter plots to showcase the relationship between dependent and independent variables. They help visualize how changes in the independent variables impact the dependent variable. Each point on the scatter plot represents a data instance with its independent and dependent variable values. By observing the pattern of points, you can identify trends, correlations, or potential nonlinear relationships between the variables. This aids in feature selection, understanding data distribution, and making informed decisions when building predictive models.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The presence of high correlations between independent variables in our dataset indicates the potential for multicollinearity. Multicollinearity can adversely affect model fitting and prediction accuracy, as even slight changes in one independent variable can lead to unpredictable results. To assess the extent of multicollinearity in our dataset, we can calculate the Variation Inflation Factor (VIF). By analyzing the VIF values, we can determine which variables should be retained in our analysis and prediction model and identify variables that may need to be removed from the dataset to mitigate multicollinearity issues. This evaluation helps ensure the robustness and reliability of our models and supports accurate predictions and interpretations of the relationships between variables."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The impact of multicollinearity in a business context can be significant. Here are a few business implications:\n",
        "\n",
        "Model Reliability: Multicollinearity affects predictive model reliability by making it hard to isolate individual variable impacts, leading to less trustworthy predictions.\n",
        "\n",
        "Interpretation of Results: It complicates regression coefficient interpretation, hindering identification of key drivers for informed decisions.\n",
        "\n",
        "Overfitting and Generalization: Multicollinearity raises overfitting risk, causing models to struggle with new data, potentially leading to flawed strategies.\n",
        "\n",
        "Resource Allocation: Highly correlated variables might inefficiently use resources. Identifying these helps optimize allocation for relevant predictors.\n",
        "\n",
        "Risk Assessment: Models with multicollinearity can misguide decisions. Awareness is vital for proper risk assessment and mitigation strategies."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "plt.figure(figsize=(13,9))\n",
        "cor= sns.heatmap(stock_df.corr(), annot=True)"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation heatmap is an effective way to visualize the pairwise correlations between numerical variables in a dataset. It uses color coding to represent the strength and direction of the correlations, making it easier to identify patterns and relationships. By using a heatmap, it allows for a quick and intuitive understanding of the correlation structure of the variables."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every feature is extremely corelated with each other, so taking just one feature or average of these features would suffice for our regression model as linear regression assumes there is no multi colinearity in the features."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Pair Plot visualization code\n",
        "sns.pairplot(stock_df)"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pair plot is suitable when you want to visualize the relationships between multiple variables in a dataset. It creates a grid of scatter plots, making it easier to identify patterns, trends, and potential outliers. The pair plot allows for a comprehensive examination of the pairwise relationships, helping to understand how variables interact with each other. On the other hand, the pair plot provides a more comprehensive view of the relationships by displaying scatter plots for all possible variable combinations."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights reveal strong correlations among Open, High, Low, and Close variables, indicating their close ties in Yes Bank's stock. Similarly, Open, High, and Low variables are tightly correlated, suggesting synchronized trends. These correlations are valuable for prediction and decision-making, showcasing stock market interdependencies. Yet, remember, correlation doesn't mean causation. In-depth analysis considers more factors for accurate predictions and decisions."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "#Check for missing values\n",
        "print(stock_df.isnull().sum())\n",
        "\n",
        "#Drop rows with missing values\n",
        "stock_df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dropping Rows with Missing Values:**This technique removes rows that contain missing values. It is suitable when the missing values are random and do not significantly impact the overall dataset. Dropping rows can be appropriate when the missing data is relatively small compared to the available dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure with a size of 10x6 inches\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "\n",
        "\n",
        "\n",
        "# Add a super title to the plot\n",
        "plt.suptitle('Studying the Outliers after Log Transformation', color='black', fontsize=16)\n",
        "\n",
        "# Define a list of colors for the boxplots\n",
        "color_list = ['blue', 'green', 'red', 'grey']\n",
        "\n",
        "# Iterate over each column in the dataframe\n",
        "for i, column in enumerate(stock_df.columns):\n",
        "    # Create subplots for each column\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "\n",
        "    # Apply a log transformation to the column and create a boxplot\n",
        "    sns.boxplot(x=np.log10(stock_df[column]), color=color_list[i])\n",
        "\n",
        "\n",
        "\n",
        "    # Add a title to each subplot\n",
        "    plt.title(column, color='Black')\n",
        "\n",
        "# Adjust the layout of the subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WXu2k8HPAdJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The log transformation was applied as a treatment for outliers. This approach not only addresses outliers but also helps to alleviate skewness in the features' distribution. By using log transformation, two problems - outlier treatment and skewness correction - are tackled simultaneously, providing a consolidated solution. This technique aids in normalizing the data and improving the suitability of the features for analysis and modeling purposes."
      ],
      "metadata": {
        "id": "2tGUXlRLByau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our data set all the columns are numerical, including the 'Date' column being represented as an object type. Since all the columns are already numerical, there is no need for categorical encoding in this particular dataset.\n",
        "\n",
        "Categorical encoding is typically required when you have categorical variables that need to be converted into numerical representations for analysis or machine learning tasks. In our case, all the columns ('Open', 'High', 'Low', 'Close') are numerical, representing different aspects of the stock closing price"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Feature Manipulation & Selection**"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Feature Manipulation**"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Create an empty dataframe to store the VIF for each feature\n",
        "vif_df = pd.DataFrame()\n",
        "\n",
        "# Assign the feature names to the 'Features' column\n",
        "vif_df['Features'] = stock_df.iloc[:, :-1].columns.tolist()\n",
        "\n",
        "# Calculate the VIF for each feature and store it in the 'VIF' column\n",
        "vif_df['VIF'] = [variance_inflation_factor(stock_df.iloc[:, :-1].values, i) for i in range(len(stock_df.iloc[:, :-1].columns))]\n",
        "\n",
        "# Display the dataframe containing the features and their corresponding VIF values\n",
        "vif_df"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The VIF values for all the features indicate high multicollinearity. However, considering the small size of the dataset and having only three numerical independent variables, there is limited potential for feature manipulation that could be beneficial. With the absence of categorical variables, the scope for feature engineering or transformation is constrained. Therefore, the focus should be on alternative modeling approaches or additional data collection to address the issue of multicollinearity."
      ],
      "metadata": {
        "id": "N4GGRTl450L5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Feature Selection**"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Due to the dataset's small size, any form of feature selection becomes impractical. Given the limited number of observations, attempting to reduce the feature space may lead to unreliable or biased results. Therefore, it is advisable to retain all available features for analysis or modeling purposes.**"
      ],
      "metadata": {
        "id": "Zoq_T__-6G6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Data Transformation**"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To address the skewed distribution of the features, a data transformation is necessary to approximate a normal distribution. In this case, a log transformation will be applied. This transformation aims to reduce skewness and make the data more symmetrical. Furthermore, as observed earlier, the log transformation also aids in handling outliers. By employing this transformation, we can simultaneously improve the normality of the data distribution and mitigate the impact of outliers."
      ],
      "metadata": {
        "id": "2S8dHR_l6aSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over each column in the dataframe\n",
        "for column in stock_df.columns:\n",
        "    # Apply a log transformation to the column using np.log10()\n",
        "    stock_df[column] = np.log10(stock_df[column])"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure with a size of 10x8 inches\n",
        "plt.figure(figsize=(10,8))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Add a super title to the plot\n",
        "plt.suptitle('Overall Distribution of Each Variable after Log Transformation', color='black', fontsize=16)\n",
        "\n",
        "color_list = ['blue', 'green', 'red', 'purple']\n",
        "\n",
        "for i, column in enumerate(stock_df.columns):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    ax1 = plt.gca()\n",
        "    sns.histplot(stock_df[column], color=color_list[i], ax=ax1)\n",
        "    ax2 = ax1.twinx()\n",
        "    sns.kdeplot(stock_df[column], color=color_list[i], ax=ax2)  # Overlapping the KDE plot on the histogram.\n",
        "\n",
        "\n",
        "\n",
        "    # Add gridlines\n",
        "    plt.grid(which='major', alpha=0.5)\n",
        "    plt.grid(which='minor', alpha=0.5)\n",
        "\n",
        "    # Add dashed lines for mean and median\n",
        "    plt.axvline(stock_df[column].mean(), color='purple', linestyle='dashed', linewidth=2.5)\n",
        "    plt.axvline(stock_df[column].median(), color='orange', linestyle='dashed', linewidth=1.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M6YDivBYmcpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mean (indicated by the purple vertical line) and the median (represented by the yellow vertical line) are nearly equal for each feature. This alignment suggests that the log transformation successfully reduced the skewness and brought the data closer to symmetry. The convergence of the mean and median highlights the relative balance in the distribution, indicating a more representative central tendency. Overall, these observations indicate an improved approximation to a normal distribution after the log transformation."
      ],
      "metadata": {
        "id": "w5uyLY3oRARi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "##**6. Dimesionality Reduction**"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the dataset is already small in size, there is no need for dimensionality reduction techniques. With a limited number of observations, attempting to reduce the number of features may not provide significant benefits and could potentially lead to loss of valuable information. Therefore, it is advisable to retain all the available features for analysis or modeling purposes without applying dimensionality reduction methods."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Data Splitting**"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assign the independent and dependent variables to X and y, respectively\n",
        "X = stock_df[independent_variables]\n",
        "y = stock_df[dependent_variable]\n",
        "\n",
        "# Split the data into training and testing datasets using a test size of 0.2 (20%)\n",
        "# Set random_state to 0 for reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the model effectively, an 80:20 split ratio is being employed, allocating 80% of the data for training and 20% for testing. However, considering the small dataset size, it may be beneficial to acquire more data for training purposes. Increasing the training data size helps improve the model's ability to learn and generalize from the patterns present in the data. Gathering additional data can enhance the model's performance, reduce the risk of overfitting, and provide a more comprehensive representation of the underlying relationships within the dataset."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8. Data Scaling**"
      ],
      "metadata": {
        "id": "VyAdVG6VLgKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create an instance of StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale the training data (X_train) using fit_transform\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# Scale the testing data (X_test) using transform\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "FEbftBejKbJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the training dataset\n",
        "X_train[0: 10]"
      ],
      "metadata": {
        "id": "LMLuhPJsK8Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the test dataset\n",
        "X_test[0: 10]"
      ],
      "metadata": {
        "id": "tblw2GymLAoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "0JYPqSCMLByB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the StandardScaler method to preprocess our data for linear regression analysis. StandardScaler is chosen because linear regression assumes normally distributed features. By applying StandardScaler, we transform the features to have a mean of 0 and a standard deviation of 1. This standardization aligns with the assumptions of linear regression, ensuring that features are on a similar scale. This, in turn, facilitates accurate model fitting and interpretation."
      ],
      "metadata": {
        "id": "rez3Tg7iLOqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**ML Model 1 - Linear Regression**"
      ],
      "metadata": {
        "id": "DE-61PJe0OHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression is a powerful machine learning algorithm that falls under the category of supervised learning. It is specifically designed for regression tasks, where the goal is to predict a continuous target variable based on independent variables. In regression analysis, the algorithm establishes a relationship between the predictor variables and the target variable to make accurate predictions.\n",
        "\n",
        "The primary objective of Linear Regression is to identify and quantify the relationship between variables. By examining the patterns and trends in the data, the algorithm enables us to understand how changes in one variable affect the target variable. This understanding is crucial for making informed decisions and forecasting future outcomes.\n",
        "\n",
        "Linear Regression is widely employed in various domains, including finance, economics, social sciences, and engineering. It finds applications in areas such as sales forecasting, housing price prediction, demand estimation, and trend analysis. By leveraging the insights gained from analyzing the relationship between variables, Linear Regression empowers us to make reliable forecasts and make informed business decisions.\n",
        "\n",
        "In summary, Linear Regression is a versatile algorithm that allows us to explore the relationships between variables and make predictions based on those relationships. Its ability to model the dependencies between variables makes it a valuable tool for understanding data and making accurate forecasts in numerous fields."
      ],
      "metadata": {
        "id": "JgAmCVXL0vpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create an instance of the LinearRegression model\n",
        "linear_reg = LinearRegression()\n",
        "\n",
        "# Fit the Linear Regression model to the training data\n",
        "linear_reg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "iP3t_NtJ02Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "y_pred_lin = linear_reg.predict(X_test)"
      ],
      "metadata": {
        "id": "FTZeN-L91Ra2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the model parameters\n",
        "print(\"Coefficients:\", linear_reg.coef_)\n",
        "print(\"Intercept:\", linear_reg.intercept_)\n"
      ],
      "metadata": {
        "id": "BwoYG4ex1UY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "\n",
        "# Plot the actual Close prices from the test data\n",
        "plt.plot(np.array(10**y_test), color='blue')\n",
        "\n",
        "# Plot the predicted Close prices from the Linear Regression model\n",
        "plt.plot(10**y_pred_lin, color='red')\n",
        "\n",
        "# Set the label for the y-axis\n",
        "plt.ylabel(\"Close Price\")\n",
        "\n",
        "# Add a legend to differentiate between the actual and predicted values\n",
        "plt.legend([\"Actual\", \"Predicted\"])\n",
        "\n",
        "# Set the title of the plot\n",
        "plt.title(\"Linear Regression\", color='white')\n",
        "\n",
        "# Add gridlines\n",
        "plt.grid(which='major', alpha=0.5)\n",
        "plt.grid(which='minor', alpha=0.5)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6jFG0Vcl1ZdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "k_vb6Md72bSY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression aims to establish a linear connection between the independent and dependent variables by minimizing the sum of squared differences between the observed and predicted dependent values. It assumes a linear relationship and calculates the best-fitting line by adjusting the model's coefficients. The objective is to minimize the overall distance between the observed data points and the line of best fit. This approach enables the model to capture the underlying linear pattern and make predictions based on the learned relationship between the variables."
      ],
      "metadata": {
        "id": "41mn-xej2ZRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Calculate the Mean Squared Error (MSE)\n",
        "mse_lin = round(mean_squared_error(10**y_test, 10**y_pred_lin), 4)\n",
        "\n",
        "# Calculate the Root Mean Squared Error (RMSE)\n",
        "rmse_lin = round(np.sqrt(mse_lin), 4)\n",
        "\n",
        "# Calculate the Mean Absolute Error (MAE)\n",
        "mae_lin = round(mean_absolute_error(10**y_test, 10**y_pred_lin), 4)\n",
        "\n",
        "# Calculate the R-squared Score (R2)\n",
        "r2_lin = round(r2_score(10**y_test, 10**y_pred_lin), 4)\n",
        "\n",
        "# Calculate the Adjusted R-squared Score (Adjusted R2)\n",
        "adj_r2_lin = round(1 - (1 - r2_lin) * ((X_test.shape[0] - 1) / (X_test.shape[0] - X_test.shape[1] - 1)), 4)\n"
      ],
      "metadata": {
        "id": "hRwp4VWS2fov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the evaluation metrics\n",
        "evametdf_lin = pd.DataFrame()\n",
        "\n",
        "# Set the 'Metrics' column in the dataframe\n",
        "evametdf_lin['Metrics'] = ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R-2 Score', 'Adjusted R-2 Score']\n",
        "\n",
        "# Set the 'Linear Regression' column in the dataframe with the corresponding metric values\n",
        "evametdf_lin['Linear Regression'] = [mse_lin, rmse_lin, mae_lin, r2_lin, adj_r2_lin]\n",
        "\n",
        "# Display the dataframe\n",
        "evametdf_lin"
      ],
      "metadata": {
        "id": "lC-eHjTO2pzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluation metrics for the Linear Regression model are as follows:\n",
        "\n",
        "Mean Squared Error (MSE): The MSE value is 70.4204, indicating the average squared difference between the actual and predicted Close prices. Lower values indicate better model performance, as they represent a smaller overall prediction error.\n",
        "\n",
        "Root Mean Squared Error (RMSE): The RMSE value is 8.3917, which is the square root of the MSE. It provides a measure of the average difference between the actual and predicted Close prices in the original scale. Again, a lower value signifies better predictive accuracy.\n",
        "\n",
        "Mean Absolute Error (MAE): The MAE value is 4.8168, representing the average absolute difference between the actual and predicted Close prices. Similar to MSE and RMSE, a smaller MAE indicates better model performance.\n",
        "\n",
        "R-2 Score: The R-2 score is 0.9937, reflecting the proportion of variance in the dependent variable (Close prices) explained by the independent variables. A score closer to 1 indicates a better fit of the model to the data.\n",
        "\n",
        "Adjusted R-2 Score: The adjusted R-2 score is 0.9931, which considers the number of independent variables and sample size when assessing the model's goodness of fit. This adjustment helps mitigate potential overfitting issues and provides a more reliable measure of model performance.\n",
        "\n",
        "These evaluation metrics collectively demonstrate that the Linear Regression model performs well in predicting the Close prices, with low errors, a high R-2 score, and a relatively stable adjusted R-2 score."
      ],
      "metadata": {
        "id": "P8xixlz53shm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ML Model 2- Lasso Regression**"
      ],
      "metadata": {
        "id": "HWFQ9uHn30ML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso regression, also known as Penalized regression, is a machine learning method commonly used for variable selection. It offers improved prediction accuracy compared to other regression models. By applying Lasso regularization, the model can enhance interpretability while effectively reducing the impact of less relevant variables. This regularization technique plays a crucial role in feature selection and contributes to a more accurate and interpretable model."
      ],
      "metadata": {
        "id": "mENhSYhS4HHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ML Model - 2 Implementation\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso = Lasso(alpha = 0.01)\n",
        "\n",
        "# Fit the Algorithm\n",
        "lasso.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "ho1RV6KL4L5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Predict on the model\n",
        "y_pred_lasso = lasso.predict(X_test)\n",
        "\n",
        "\n",
        "# Print the coefficients of the Lasso model\n",
        "print(\"Coefficients:\", lasso.coef_)\n",
        "\n",
        "# Print the intercept of the Lasso model\n",
        "print(\"Intercept:\", lasso.intercept_)\n"
      ],
      "metadata": {
        "id": "4TfB1GXS4SFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "\n",
        "# Plot the actual Close prices from the test data\n",
        "plt.plot(np.array(10**y_test), color='blue')\n",
        "\n",
        "# Plot the predicted Close prices from the Lasso Regression model\n",
        "plt.plot(10**y_pred_lasso, color='red')\n",
        "\n",
        "# Set the label for the y-axis\n",
        "plt.ylabel(\"Close Price\")\n",
        "\n",
        "# Add a legend to differentiate between the actual and predicted values\n",
        "plt.legend([\"Actual\", \"Predicted\"])\n",
        "\n",
        "# Set the title of the plot\n",
        "plt.title(\"Lasso Regression\", color='white')\n",
        "\n",
        "# Add gridlines\n",
        "plt.grid(which='major', alpha=0.5)\n",
        "plt.grid(which='minor', alpha=0.5)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yezuNzv84Zec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.**"
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso Regression is a regularization technique employed in Linear Regression models. It incorporates a penalty term into the loss function that is based on the sum of the absolute values of the coefficients. This penalty term encourages sparsity in the model by driving some coefficients to exactly zero. As a result, Lasso Regression not only reduces the magnitudes of the coefficients but can also eliminate some features from the model by setting their corresponding coefficients to zero.\n",
        "\n",
        "By reducing the coefficients to zero, Lasso Regression performs feature selection, effectively identifying and prioritizing the most important features for predicting the target variable. This characteristic makes Lasso Regression particularly useful when dealing with high-dimensional datasets where feature reduction is desired.\n",
        "\n",
        "The regularization effect of Lasso Regression helps mitigate overfitting by preventing the model from relying too heavily on any individual feature. It encourages a more parsimonious model representation, improving its generalizability to unseen data. The capability of Lasso Regression to shrink coefficients towards zero and perform feature selection makes it a valuable tool for both improving model interpretability and enhancing prediction accuracy."
      ],
      "metadata": {
        "id": "-9xyPCTijRdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Mean Squared Error\n",
        "mse_lasso = round( mean_squared_error((10**y_test), 10**(y_pred_lasso)), 4)\n",
        "\n",
        "# Root Mean Squared Error\n",
        "rmse_lasso = round(np.sqrt(mse_lasso), 4)\n",
        "\n",
        "# Mean Absolute Error\n",
        "mae_lasso = round(mean_absolute_error((10**y_test), 10**(y_pred_lasso)), 4)\n",
        "\n",
        "# R-2 Score\n",
        "r2_lasso = round(r2_score((10**y_test), (10**y_pred_lasso)), 4)\n",
        "\n",
        "# Adjusted R-2 Score\n",
        "adj_r2_lasso = round(1 - (1 - r2_lasso)*((X_test.shape[0] - 1)/(X_test.shape[0] - X_test.shape[1] - 1)), 4)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the evaluation metrics for Lasso Regression\n",
        "evametdf_lasso = pd.DataFrame()\n",
        "\n",
        "# Set the 'Metrics' column in the dataframe\n",
        "evametdf_lasso['Metrics'] = ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R-2 Score', 'Adjusted R-2 Score']\n",
        "\n",
        "# Set the 'Lasso Regression' column in the dataframe with the corresponding metric values\n",
        "evametdf_lasso['Lasso Regression'] = [mse_lasso, rmse_lasso, mae_lasso, r2_lasso, adj_r2_lasso]\n",
        "\n",
        "# Display the dataframe\n",
        "evametdf_lasso"
      ],
      "metadata": {
        "id": "WPyJFYeajg46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Cross- Validation & Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the hyperparameter grid for Lasso Regression\n",
        "lasso_param_grid = {'alpha': [0.00001, 0.0001, 0.001, 0.01, 1, 10, 100, 1000]}\n",
        "\n",
        "# Perform GridSearchCV with Lasso Regression\n",
        "lasso_gscv = GridSearchCV(lasso, param_grid=lasso_param_grid, scoring='neg_mean_squared_error', cv=3)\n",
        "\n",
        "# Fit the Lasso Regression model with GridSearchCV\n",
        "lasso_gscv.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "_qnOJSukjsoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the best parameter value\n",
        "print(\"The best value of 'alpha' would be:\", lasso_gscv.best_params_)"
      ],
      "metadata": {
        "id": "vcd1jYSgj2wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the coefficients of the best estimator from GridSearchCV\n",
        "print(\"Coefficients:\", lasso_gscv.best_estimator_.coef_)\n",
        "\n",
        "# Print the intercept of the best estimator from GridSearchCV\n",
        "print(\"Intercept:\", lasso_gscv.best_estimator_.intercept_)"
      ],
      "metadata": {
        "id": "IBJVhSZdj6sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "y_pred_lasso_gscv = lasso_gscv.predict(X_test)"
      ],
      "metadata": {
        "id": "AOPvuaqej_ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# Set the background colors for the figure\n",
        "plot_bgcolor = (36/255, 40/255, 47/255, 1)  # RGB values divided by 255, with alpha=1 for full opacity\n",
        "\n",
        "\n",
        "\n",
        "# Plot the actual Close prices from the test data\n",
        "plt.plot(np.array(10**y_test), color='blue')\n",
        "\n",
        "# Plot the predicted Close prices from the Lasso Regression model with GridSearchCV\n",
        "plt.plot(10**y_pred_lasso_gscv, color='red')\n",
        "\n",
        "# Set the label for the y-axis\n",
        "plt.ylabel(\"Close Price\")\n",
        "\n",
        "# Add a legend to differentiate between the actual and predicted values\n",
        "plt.legend([\"Actual\", \"Predicted\"])\n",
        "\n",
        "# Set the title of the plot\n",
        "plt.title(\"Lasso Regression with GridSearchCV\", color='white')\n",
        "\n",
        "# Add gridlines\n",
        "plt.grid(which='major', alpha=0.5)\n",
        "plt.grid(which='minor', alpha=0.5)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MqXBwXY-kFLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Squared Error\n",
        "mse_lasso_gscv = round( mean_squared_error((10**y_test), 10**(y_pred_lasso_gscv)), 4)\n",
        "\n",
        "# Root Mean Squared Error\n",
        "rmse_lasso_gscv = round(np.sqrt(mse_lasso_gscv), 4)\n",
        "\n",
        "# Mean Absolute Error\n",
        "mae_lasso_gscv = round(mean_absolute_error((10**y_test), 10**(y_pred_lasso_gscv)), 4)\n",
        "\n",
        "# R-2 Score\n",
        "r2_lasso_gscv = round(r2_score((10**y_test), (10**y_pred_lasso_gscv)), 4)\n",
        "\n",
        "# Adjusted R-2 Score\n",
        "adj_r2_lasso_gscv = round(1 - (1 - r2_lasso_gscv)*((X_test.shape[0] - 1)/(X_test.shape[0] - X_test.shape[1] - 1)), 4)\n"
      ],
      "metadata": {
        "id": "ZkDPMcALkRKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the evaluation metrics for Lasso Regression with GridSearchCV\n",
        "evametdf_lasso_gscv = pd.DataFrame()\n",
        "\n",
        "# Add the column \"Metrics\" to the dataframe\n",
        "evametdf_lasso_gscv['Metrics'] = ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R-2 Score', 'Adjusted R-2 Score']\n",
        "\n",
        "# Add the column \"Lasso Regression with GridSearchCV\" to the dataframe with the corresponding evaluation metric values\n",
        "evametdf_lasso_gscv['Lasso Regression with GridSearchCV'] = [mse_lasso_gscv, rmse_lasso_gscv, mae_lasso_gscv, r2_lasso_gscv, adj_r2_lasso_gscv]\n",
        "\n",
        "# Display the dataframe\n",
        "evametdf_lasso_gscv\n"
      ],
      "metadata": {
        "id": "Mul9p1ogaLXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV was used with a smaller set of hyperparameters to find the best combination of hyperparameter values for Lasso Regression. The hyperparameter grid specified a range of alpha values. By narrowing down the set of hyperparameters, the search space was reduced, making the grid search more efficient. GridSearchCV then performed cross-validation to evaluate the performance of each combination of hyperparameters based on the negative mean squared error. The best set of hyperparameters was determined based on the highest cross-validated score, resulting in the optimal regularization strength for Lasso Regression. This approach allowed for an effective and efficient search for the optimal hyperparameters and minimized the mean squared error."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the comparison of evaluation metrics for Lasso Regression and Lasso Regression with GridSearchCV\n",
        "lasso_comp_df = pd.concat([evametdf_lasso, evametdf_lasso_gscv.iloc[:, 1]], axis=1)\n",
        "\n",
        "# Display the dataframe\n",
        "lasso_comp_df"
      ],
      "metadata": {
        "id": "cOvr-Rdz6cHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso Regression with GridSearchCV is considered the winner due to its lower error metrics and slightly higher R-2 scores. The lower mean squared error, root mean squared error, and mean absolute error indicate improved accuracy and better predictive performance compared to Lasso Regression without GridSearchCV. Additionally, the slightly higher R-2 score suggests that Lasso Regression with GridSearchCV captures a greater amount of variance in the target variable and provides a better fit to the data. Overall, these evaluation metrics demonstrate that Lasso Regression with GridSearchCV outperforms Lasso Regression without GridSearchCV in terms of predictive accuracy and model fit."
      ],
      "metadata": {
        "id": "HI9s9zHd6NP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3  **Ridge Regression**"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge regression is a regularization technique used in multiple regression analysis. While it may seem daunting at first, gaining a solid understanding of multiple regression can provide a foundation for comprehending the science behind Ridge regression in R.\n",
        "\n",
        "In multiple regression, the goal is to build a model that predicts the relationship between a dependent variable and multiple independent variables. This is done by estimating the coefficients of the independent variables that minimize the difference between the predicted and actual values of the dependent variable. The traditional least squares method is commonly used to estimate these coefficients.\n",
        "\n",
        "Ridge regression, on the other hand, introduces a regularization term to the least squares method. This regularization term, known as the Ridge penalty or L2 regularization, adds a constraint to the coefficient estimation process. The purpose of this constraint is to prevent overfitting and improve the model's generalization ability.\n",
        "\n",
        "The Ridge penalty works by adding a weighted sum of squared coefficients to the ordinary least squares cost function. This sum penalizes larger coefficient values, encouraging them to be smaller. Consequently, Ridge regression tends to shrink the coefficient estimates towards zero, while still allowing them to have non-zero values. This shrinkage effect helps mitigate the impact of multicollinearity, a situation where the independent variables are highly correlated with each other.\n",
        "\n",
        "In R, implementing Ridge regression involves specifying a tuning parameter, often denoted as lambda or alpha. This parameter controls the amount of regularization applied to the model. A larger lambda value results in stronger regularization, leading to smaller coefficient estimates. Conversely, a smaller lambda value reduces the regularization effect, allowing the coefficients to approach the values obtained from ordinary least squares regression.\n",
        "\n",
        "By understanding the fundamentals of multiple regression, researchers can grasp the underlying principles of Ridge regression in R. This regularization technique offers a valuable tool for handling multicollinearity and improving the generalization performance of multiple regression models."
      ],
      "metadata": {
        "id": "9rfEq1IZ6V-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "# Import the Ridge regression model from scikit-learn\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Create an instance of the Ridge regression model\n",
        "ridge = Ridge()\n",
        "\n",
        "# Fit the Ridge regression model to the training data\n",
        "ridge.fit(X_train, y_train)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_ridge = ridge.predict(X_test)\n",
        "\n",
        "# Print the coefficients of the Ridge regression model\n",
        "print(\"Coefficients:\", ridge.coef_)\n",
        "\n",
        "# Print the intercept of the Ridge regression model\n",
        "print(\"Intercept:\", ridge.intercept_)\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# Set the background colors for the figure\n",
        "plot_bgcolor = (36/255, 40/255, 47/255, 1)  # RGB values divided by 255, with alpha=1 for full opacity\n",
        "\n",
        "\n",
        "\n",
        "# Plot the actual Close prices from the test data in blue\n",
        "plt.plot(np.array(10**y_test), color='blue')\n",
        "\n",
        "# Plot the predicted Close prices from the Ridge regression model in red\n",
        "plt.plot(10**y_pred_ridge, color='red')\n",
        "\n",
        "# Set the label for the y-axis as \"Close Price\"\n",
        "plt.ylabel(\"Close Price\")\n",
        "\n",
        "# Add a legend to differentiate between the actual and predicted values\n",
        "plt.legend([\"Actual\", \"Predicted\"])\n",
        "\n",
        "# Set the title of the plot as \"Ridge Regression\" with white color\n",
        "plt.title(\"Ridge Regression\", color='Black')\n",
        "\n",
        "# Add grid lines to the plot\n",
        "plt.grid(which='major', alpha=0.5)\n",
        "plt.grid(which='minor', alpha=0.5)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BY8O554B6xHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge Regression is a regularization technique used in Linear Regression models. It introduces a penalty term to the loss function, which is the sum of squared values of the coefficients. This penalty term helps control the magnitude of the coefficients, limiting their impact on the model and reducing the chances of overfitting. By adding this penalty term, Ridge Regression encourages a balance between fitting the training data well and maintaining generalization to unseen data. It is an effective approach to handle multicollinearity and stabilize the model's performance."
      ],
      "metadata": {
        "id": "vgT_7TPW63dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculate the Mean Squared Error (MSE)\n",
        "mse_ridge = round(mean_squared_error(10**y_test, 10**y_pred_ridge), 4)\n",
        "\n",
        "# Calculate the Root Mean Squared Error (RMSE)\n",
        "rmse_ridge = round(np.sqrt(mse_ridge), 4)\n",
        "\n",
        "# Calculate the Mean Absolute Error (MAE)\n",
        "mae_ridge = round(mean_absolute_error(10**y_test, 10**y_pred_ridge), 4)\n",
        "\n",
        "# Calculate the R-squared Score (R2 Score)\n",
        "r2_ridge = round(r2_score(10**y_test, 10**y_pred_ridge), 4)\n",
        "\n",
        "# Calculate the Adjusted R-squared Score (Adjusted R2 Score)\n",
        "adj_r2_ridge = round(1 - (1 - r2_ridge) * ((X_test.shape[0] - 1) / (X_test.shape[0] - X_test.shape[1] - 1)), 4)\n",
        "\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to store the evaluation metrics\n",
        "evametdf_ridge = pd.DataFrame()\n",
        "\n",
        "# Set the metrics as a column in the dataframe\n",
        "evametdf_ridge['Metrics'] = ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R-2 Score', 'Adjusted R-2 Score']\n",
        "\n",
        "# Set the corresponding values for Ridge Regression in the dataframe\n",
        "evametdf_ridge['Ridge Regression'] = [mse_ridge, rmse_ridge, mae_ridge, r2_ridge, adj_r2_ridge]\n",
        "\n",
        "evametdf_ridge"
      ],
      "metadata": {
        "id": "RyX5hUM27Fxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid for Ridge regression\n",
        "ridge_param_grid = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n",
        "\n",
        "# Create an instance of the Ridge regression model\n",
        "ridge = Ridge()\n",
        "\n",
        "# Create an instance of GridSearchCV with the Ridge regression model,\n",
        "# the hyperparameter grid, scoring metric, and cross-validation settings\n",
        "ridge_gscv = GridSearchCV(ridge, param_grid=ridge_param_grid, scoring='neg_mean_squared_error', cv=3)\n",
        "\n",
        "# Fit the GridSearchCV instance to the training data\n",
        "ridge_gscv.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "y_pred_ridge_gscv = ridge_gscv.predict(X_test)"
      ],
      "metadata": {
        "id": "pTlYnZUC7aLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the best parameter value\n",
        "print(\"The best value of 'alpha' would be:\", ridge_gscv.best_params_)\n"
      ],
      "metadata": {
        "id": "EMG9D6Zd7fRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Checking the model parameters after GridSearchCV\n",
        "print(\"Coefficients:\", ridge_gscv.best_estimator_.coef_)\n",
        "print(\"Intercept:\", ridge_gscv.best_estimator_.intercept_)"
      ],
      "metadata": {
        "id": "mskXZleg7iQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# Set the background colors for the figure\n",
        "plot_bgcolor = (36/255, 40/255, 47/255, 1)  # RGB values divided by 255, with alpha=1 for full opacity\n",
        "\n",
        "\n",
        "\n",
        "# Plot the actual Close prices from the test set in blue\n",
        "plt.plot(np.array(10**y_test), color='blue')\n",
        "\n",
        "# Plot the predicted Close prices from the Ridge regression model with GridSearchCV in red\n",
        "plt.plot(10**ridge_gscv.predict(X_test), color='red')\n",
        "\n",
        "# Set the y-axis label\n",
        "plt.ylabel(\"Close Price\")\n",
        "\n",
        "# Add a legend for the plotted lines\n",
        "plt.legend([\"Actual\", \"Predicted\"])\n",
        "\n",
        "# Add grid lines to the plot\n",
        "plt.grid(which='major', alpha=0.5)\n",
        "plt.grid(which='minor', alpha=0.5)\n",
        "\n",
        "# Set the title of the plot with white color\n",
        "plt.title(\"Ridge Regression with GridSearchCV\", color='white')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S6Gs7hHP7lol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Squared Error\n",
        "mse_ridge_gscv = round( mean_squared_error((10**y_test), 10**(y_pred_ridge_gscv)), 4)\n",
        "\n",
        "# Root Mean Squared Error\n",
        "rmse_ridge_gscv = round(np.sqrt(mse_ridge_gscv), 4)\n",
        "\n",
        "# Mean Absolute Error\n",
        "mae_ridge_gscv = round(mean_absolute_error((10**y_test), 10**(y_pred_ridge_gscv)), 4)\n",
        "\n",
        "# R-2 Score\n",
        "r2_ridge_gscv = round(r2_score((10**y_test), (10**y_pred_ridge_gscv)), 4)\n",
        "\n",
        "# Adjusted R-2 Score\n",
        "adj_r2_ridge_gscv = round(1 - (1 - r2_ridge_gscv)*((X_test.shape[0] - 1)/(X_test.shape[0] - X_test.shape[1] - 1)), 4)\n",
        "\n"
      ],
      "metadata": {
        "id": "6lohb3qc7uiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty dataframe\n",
        "evametdf_ridge_gscv = pd.DataFrame()\n",
        "\n",
        "# Create a column for the evaluation metrics\n",
        "evametdf_ridge_gscv['Metrics'] = ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R-2 Score', 'Adjusted R-2 Score']\n",
        "\n",
        "# Create a column for the Ridge Regression with GridSearchCV results\n",
        "evametdf_ridge_gscv['Ridge Regression with GridSearchCV'] = [mse_ridge_gscv, rmse_ridge_gscv, mae_ridge_gscv, r2_ridge_gscv, adj_r2_ridge_gscv]\n",
        "\n",
        "# Display the dataframe\n",
        "evametdf_ridge_gscv"
      ],
      "metadata": {
        "id": "oW0xiL667xjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason GridSearchCV was used in this code is that we are working with a smaller set of hyperparameters for the Ridge regression model. GridSearchCV allows us to exhaustively search through the specified hyperparameter grid and find the best combination of hyperparameters that yields the optimal model performance.\n",
        "\n",
        "In this case, the hyperparameter being tuned is the alpha parameter, which represents the regularization strength in Ridge regression. The ridge_param_grid contains a predefined list of potential alpha values to explore. By using GridSearchCV, the code iterates through each alpha value in the grid, fits the Ridge regression model with that particular alpha, and evaluates the model's performance using cross-validation.\n",
        "\n",
        "GridSearchCV is an effective approach when dealing with a smaller hyperparameter space because it systematically evaluates every possible combination within that space. However, as the hyperparameter space grows larger, GridSearchCV may become computationally expensive and time-consuming.\n",
        "\n",
        "It's important to note that the choice of hyperparameter search method depends on the specific problem, the size of the hyperparameter space, and the available computational resources. GridSearchCV is suitable for smaller hyperparameter spaces, while other techniques like RandomizedSearchCV or Bayesian optimization may be more efficient for larger hyperparameter spaces.\n",
        "\n",
        "Overall, GridSearchCV provides a systematic way to search through a smaller set of hyperparameters and identify the optimal combination for the Ridge regression model, leading to improved model performance."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenating two DataFrames side by side using pd.concat()\n",
        "# Here, we are combining 'evametdf_ridge' and the second column ('iloc[:, 1]') of 'evametdf_ridge_gscv' DataFrame.\n",
        "ridge_comp_df = pd.concat([evametdf_ridge, evametdf_ridge_gscv.iloc[:, 1]], axis=1)\n",
        "\n",
        "# Displaying the resulting DataFrame after concatenation\n",
        "ridge_comp_df\n"
      ],
      "metadata": {
        "id": "9stWgrTM8yX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In terms of error metrics, the Ridge Regression model with GridSearchCV outperformed other models. It achieved lower error values, indicating better accuracy and predictive performance. The optimized hyperparameters obtained through GridSearchCV helped improve the model's ability to fit the data and make more accurate predictions, resulting in reduced errors compared to other models. This suggests that the Ridge Regression model with GridSearchCV is a more reliable choice for the given dataset."
      ],
      "metadata": {
        "id": "uBIORbV-8-S1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the evaluation and comparison of the models' performance primarily focus on two key metrics: Root Mean Square Error (RMSE) and R-2 Score. The RMSE is a measure of the average magnitude of the prediction errors, providing insights into the models' ability to accurately estimate the Close prices. A lower RMSE indicates better predictive accuracy, as it signifies that the models' predictions are closer to the actual Close prices.\n",
        "\n",
        "The R-2 Score, also known as the coefficient of determination, quantifies the proportion of the variance in the target variable (Close prices) that is explained by the predictor variables. A higher R-2 Score indicates a better fit of the model to the data, as it suggests that a larger portion of the variation in the Close prices can be accounted for by the predictors.\n",
        "\n",
        "In this analysis, the dataset has been preprocessed to effectively handle outliers, ensuring that they do not significantly impact the models' performance. Therefore, there is no need to be concerned about the models' sensitivity to outliers.\n",
        "\n",
        "Additionally, given the small size of the dataset and the models being trained using the same predictor variables, there is no requirement to consider adjusted scores. Adjusted scores are typically used when comparing models with different sets of predictors or when dealing with larger datasets. In this case, since the models are trained on the same predictors and the dataset size is relatively small, the adjusted scores are not necessary for a meaningful comparison.\n",
        "\n",
        "By placing emphasis on RMSE and R-2 Score, we can effectively evaluate the models' predictive power and their ability to explain the variation in the Close prices. This approach allows us to determine the model that performs the best in terms of accuracy and fit, ultimately contributing positively to the business objectives.\""
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Concatenate the evaluation metric dataframes of the best performing models from each section\n",
        "overall_evametdf = pd.concat([evametdf_lin,\n",
        "                              ridge_comp_df.loc[:, 'Ridge Regression with GridSearchCV'],\n",
        "                              lasso_comp_df.loc[:, 'Lasso Regression with GridSearchCV'],\n",
        "                              ], axis=1)\n",
        "\n",
        "# Display the concatenated dataframe\n",
        "overall_evametdf\n"
      ],
      "metadata": {
        "id": "-npf1Vpo5CBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    'Linear Regression': [70.4204, 8.3917, 4.8168, 0.9937, 0.9931],\n",
        "    'Ridge Regression with GridSearchCV': [70.2044, 8.3788, 4.9692, 0.9938, 0.9932],\n",
        "    'Lasso Regression with GridSearchCV': [70.3311, 8.3864, 4.8262, 0.9938, 0.9932],\n",
        "\n",
        "}\n",
        "overall_evametdf = pd.DataFrame(data, index=['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R-2 Score', 'Adjusted R-2 Score'])\n",
        "\n",
        "# Transpose the DataFrame to have models as columns and metrics as rows\n",
        "overall_evametdf_T = overall_evametdf.T\n",
        "\n",
        "# Create a Plotly figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add trace for each metric and model\n",
        "colors = ['rgb(91, 155, 213)', 'rgb(237, 125, 49)', 'rgb(165, 165, 165)', 'rgb(112, 173, 71)', 'rgb(255, 192, 0)']\n",
        "for i, metric in enumerate(overall_evametdf_T.index):\n",
        "    fig.add_trace(go.Bar(x=overall_evametdf_T.columns, y=overall_evametdf_T.loc[metric],\n",
        "                         name=metric, marker_color=colors[i]))\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title_text='Comparison of Metrics for Different Models',\n",
        "    title_x=0.5,  # Set the title at the middle of the plot\n",
        "    xaxis_title='Models',\n",
        "    yaxis_title='Metric Values',\n",
        "    xaxis=dict(title_font=dict(size=14)),  # Increase the font size of the x-axis title to 18\n",
        "    yaxis=dict(title_font=dict(size=14)),  # Increase the font size of the y-axis title to 18\n",
        "    barmode='group',\n",
        "    legend=dict(y=1.0, bgcolor='rgba(255, 255, 255, 0.5)'),\n",
        "    width=1200,  # Increase the figure width for better visual appeal\n",
        "    height=600,  # Increase the figure height for better visual appeal\n",
        "    plot_bgcolor='rgb(36, 40, 47)',  # Set the dark blue background color of the plot\n",
        "    paper_bgcolor='rgb(51, 56, 66)',  # Set the dark blue background color of the paper area\n",
        "    font_color='white',\n",
        "    hovermode='x',  # Show hover information for each bar\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "euVh1RYq6v63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon analyzing the evaluation metrics of the different models, it is evident that the 'Ridge Regression with GridSearchCV' model stands out as the preferred choice. This conclusion is based on several factors:\n",
        "\n",
        "Comparatively lower (Root) Mean Squared Error (MSE/RMSE):\n",
        "\n",
        "- The 'Ridge Regression with GridSearchCV' model demonstrates the lowest MSE/RMSE among the evaluated models. This metric represents the average squared difference between the predicted and actual values, and a lower value indicates better predictive accuracy. Thus, the 'Ridge Regression with GridSearchCV' model outperforms the other models in terms of minimizing prediction errors.\n",
        "Dataset size and feature retention:\n",
        "\n",
        "- The small size of the dataset warrants caution when considering the Lasso Regression model. Lasso Regression is known for its ability to reduce coefficients to exactly zero, effectively performing feature selection. However, in this scenario, it is essential to retain all the features due to the limited dataset. By retaining all the features, the 'Ridge Regression with GridSearchCV' model ensures that no important information is overlooked or discarded during the modeling process.\n",
        "\n",
        "\n",
        "Based on these observations, the 'Ridge Regression with GridSearchCV' model is the recommended choice for its superior performance in terms of lower MSE/RMSE and the preservation of all features. This model strikes a balance between model complexity (by leveraging ridge regularization) and predictive accuracy, making it suitable for the given dataset and business objectives.\n",
        "\n",
        "By selecting the 'Ridge Regression with GridSearchCV' model, we can have confidence in its ability to provide accurate predictions while considering all available features in the dataset, thereby making it the optimal choice for achieving the desired business impact."
      ],
      "metadata": {
        "id": "e8_hGOF87Ec1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choosing Ridge Regression with GridSearchCV as the final prediction model\n",
        "\n",
        "final_pred_model = ridge_gscv"
      ],
      "metadata": {
        "id": "xFHWkFeD7b3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As discussed earlier, Ridge Regression is a regularization technique used in Linear Regression models to address overfitting and multicollinearity. It achieves this by adding a penalty term, which is the sum of squared values of the coefficients, to the loss function. This penalty term limits the magnitude of the coefficients, promoting a balance between the model's complexity and its ability to generalize well.\n",
        "\n",
        "On the other hand, GridSearchCV is a hyperparameter tuning technique that performs an exhaustive search over a predefined set of potential hyperparameters. It aims to find the best combination of hyperparameters for a model by evaluating them using cross-validation.\n",
        "\n",
        "When Ridge Regression is combined with GridSearchCV, Ridge Regression serves as the base model, and GridSearchCV is utilized to determine the optimal value for the regularization strength, also known as the hyperparameter controlling the penalty term.\n",
        "\n",
        "By leveraging GridSearchCV, we can systematically explore different values of the regularization strength and identify the one that yields the best performance according to the chosen evaluation metric. This approach allows us to fine-tune the Ridge Regression model and optimize its performance based on the given hyperparameters.\n",
        "\n",
        "To assess the feature importance of the Ridge Regression model, we can examine the coefficients associated with each feature. The magnitude of these coefficients indicates the relative influence of the corresponding features on the predicted outcome. By analyzing the feature importance, we gain insights into which features have the greatest impact on the predictions and can focus on interpreting their influence on the target variable."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Installing Eli5. It is a Python library used for interpreting machine learning models.\n",
        "\n",
        "! pip install eli5"
      ],
      "metadata": {
        "id": "OGBzdAog7xfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Importing the libraries\n",
        "\n",
        "import eli5\n",
        "\n",
        "# Show feature weights using eli5\n",
        "eli5.show_weights(final_pred_model.best_estimator_, feature_names=independent_variables)\n"
      ],
      "metadata": {
        "id": "d1xh3wZP73zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The 'High price' feature has a weight of +0.333, indicating a positive influence on the predictions. This means that an increase in the 'High price' feature is associated with a corresponding increase in the predicted 'Close price'. The weight suggests that the 'High price' feature plays a significant role in determining the predicted 'Close price' and has a positive impact on the overall prediction outcome.\n",
        "\n",
        "- The 'Low price' feature has a weight of +0.315, indicating a positive influence on the predictions. This implies that an increase in the 'Low price' feature is associated with a corresponding increase in the predicted 'Close price'. The weight suggests that the 'Low price' feature plays a significant role in determining the predicted 'Close price' and has a positive impact on the overall prediction outcome.\n",
        "\n",
        "- On the other hand, the 'Open price' feature has a weight of -0.226, suggesting a negative influence on the predictions. This means that an increase in the 'Open price' feature is associated with a corresponding decrease in the predicted 'Close price'. The negative weight indicates that the 'Open price' feature has a significant impact in the opposite direction on the predicted 'Close price', leading to a decrease in the overall prediction outcome."
      ],
      "metadata": {
        "id": "1_PrC8hM7_uf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**8.Hypothesis Testing**"
      ],
      "metadata": {
        "id": "ND3xU8HrcSux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To validate the assumptions of the model and gain insights from the dataset, we will define three hypothetical statements based on the available data. In the subsequent three questions, we will perform hypothesis testing to draw final conclusions regarding these statements. Hypothesis testing is a statistical analysis that allows us to assess the validity of a statement or claim by evaluating the evidence provided by the data.\n",
        "\n",
        "We will select an appropriate statistical test based on the nature of the hypothesis, type of data, and assumptions involved. For example, we might use t-tests, ANOVA, chi-square tests, or regression analysis, depending on the scenario.\n",
        "\n",
        "By performing hypothesis testing on the dataset, we can draw meaningful and statistically sound conclusions regarding the hypothetical statements, providing valuable insights and supporting evidence based on the available data."
      ],
      "metadata": {
        "id": "Cx6gVthKcm-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothetical Testing 1**"
      ],
      "metadata": {
        "id": "zl9bdURJdAo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this analysis, the Goldfeld-Quandt Test is employed to assess the homoscedasticity of the residuals. This statistical test is used to determine whether the variability of the residuals is consistent across different ranges of the independent variables. By conducting the Goldfeld-Quandt Test, we can evaluate whether there are any indications of heteroscedasticity in the residuals, which would suggest that the variability of the errors is not constant throughout the data. Homoscedasticity, on the other hand, implies that the residuals have a consistent level of variability across the independent variables"
      ],
      "metadata": {
        "id": "GkYrzF9gdGzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. State Your research hypothesis as a null hypothesis and alternate hypothesis.**"
      ],
      "metadata": {
        "id": "QjjeIUILdM2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of assessing the homoscedasticity of residuals, we define the null hypothesis (H0) as stating that the residuals exhibit homoscedasticity, meaning that their variability is constant.\n",
        "\n",
        "The alternative hypothesis (Ha) posits that the residuals are heteroscedastic, implying that their variability differs across the range of the independent variables.\n",
        "\n",
        "Through hypothesis testing, we aim to evaluate the evidence provided by the data to determine whether we have sufficient statistical support to reject the null hypothesis in favor of the alternative hypothesis. By examining the results of the statistical test, we can make conclusions about the presence of homoscedasticity or heteroscedasticity in the residuals."
      ],
      "metadata": {
        "id": "PO-9pcp5dTf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries\n",
        "\n",
        "from statsmodels.stats.diagnostic import het_goldfeldquandt\n",
        "\n",
        "# Calculate the residuals\n",
        "residuals = 10**y_test.values - 10**y_pred_ridge_gscv.reshape(-1, 1)\n",
        "\n",
        "# Perform the Goldfeld-Quandt test to check for homoscedasticity of the residuals\n",
        "p_value = het_goldfeldquandt(residuals, y_pred_ridge_gscv.reshape(-1, 1))[1]\n",
        "\n",
        "# Interpret the results\n",
        "if p_value < 0.05:\n",
        "    print('The residuals are heteroscedastic (Reject Null Hypothesis).')\n",
        "else:\n",
        "    print('The residuals are homoscedastic (Accept Null Hypothesis).')\n",
        "\n",
        "print(f'\\nThe p-value is: {p_value}')"
      ],
      "metadata": {
        "id": "sX4fAykXdbHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the key assumptions of linear regression is that the residuals, or the differences between the observed and predicted values, should exhibit homoscedasticity. Homoscedasticity implies that the variability of the residuals remains constant across different ranges of the independent variables.\n",
        "\n",
        "To validate this assumption, we performed the Goldfeld-Quandt test on the residuals of the linear regression model. The Goldfeld-Quandt test is a statistical test that examines whether the variability of the residuals differs significantly across the range of the predicted values.\n",
        "\n",
        "After conducting the test and analyzing the results, we found that the p-value was greater than the significance level (e.g., 0.05), indicating that we do not have sufficient evidence to reject the null hypothesis. Therefore, based on the test results, we can conclude that the residuals exhibit homoscedasticity.\n",
        "\n",
        "This suggests that the assumption of homoscedasticity, which assumes that the variability of the residuals remains constant, holds true for our linear regression model. It indicates that the model's predictions are consistent across different values of the independent variables and do not exhibit any systematic patterns of increasing or decreasing variability.\n",
        "\n"
      ],
      "metadata": {
        "id": "td5HlE5ydi-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why did you choose the specific statistical test?**"
      ],
      "metadata": {
        "id": "6UM6pMCedzso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Goldfeld-Quandt Test is a widely used statistical test, particularly in regression analysis, to assess the presence of heteroscedasticity in the data. Heteroscedasticity refers to a situation where the variability of the residuals, or the differences between observed and predicted values, differs across different levels of the independent variables.\n",
        "\n",
        "By applying the Goldfeld-Quandt Test, we can evaluate whether the assumption of homoscedasticity, which assumes constant variability of the residuals, holds true for our regression analysis. The test examines if there is evidence of systematic patterns in the variability of the residuals across different ranges of the independent variables.\n",
        "\n",
        "In our analysis, we performed the Goldfeld-Quandt Test on the residuals obtained from the regression model. By analyzing the results, we can determine if there is statistical significance to support the presence of heteroscedasticity in the data.\n",
        "\n",
        "By utilizing the Goldfeld-Quandt Test, we gain insights into the nature of the variability in the residuals, allowing us to assess the adequacy of the assumption of homoscedasticity. This helps us to better understand the properties of our regression model and make more reliable inferences from the analysis."
      ],
      "metadata": {
        "id": "10QPdUkxd5E0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothetical Statement - 2**"
      ],
      "metadata": {
        "id": "Hy1oYVyYd-4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. State Your research hypothesis as a null hypothesis and alternate hypothesis.**"
      ],
      "metadata": {
        "id": "RYMqrxiKeDMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this analysis, we employed the Ljung-Box Test to examine the presence of autocorrelation among the residuals. Autocorrelation refers to the correlation of a variable with its lagged values, indicating whether there is a pattern or relationship between the residuals at different time points.\n",
        "\n",
        "By conducting the Ljung-Box Test, we aimed to assess whether there is any statistically significant autocorrelation present in the residuals of our analysis. This test helps us determine whether the assumption of independence between the residuals holds true or if there are any systematic patterns or dependencies in the data.\n",
        "\n",
        "Null Hypothesis, H<sub>0</sub> : Autocorrelation is absent among the residuals.\n",
        "\n",
        "Alternate Hypothesis, H<sub>A</sub>: Autocorrelation is present among the residuals."
      ],
      "metadata": {
        "id": "f0sC4uC5eJKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Perform an appropriate statistical test.**"
      ],
      "metadata": {
        "id": "wv6U5Y28ek0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries\n",
        "\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "\n",
        "# Perform the Ljung-Box test to check for autocorrelation among the residuals\n",
        "p_values = acorr_ljungbox(residuals)['lb_pvalue']\n",
        "\n",
        "# Choose the minimum p-value (as all the residual values are tested)\n",
        "p = min(p_values)\n",
        "\n",
        "# Interpret the results\n",
        "if p < 0.05:\n",
        "    print('Autocorrelation is present among the residuals (Reject Null Hypothesis).')\n",
        "else:\n",
        "    print('Autocorrelation is absent among the residuals (Accept Null Hypothesis).')\n",
        "\n",
        "print(f'\\nThe p-value is: {p}')\n"
      ],
      "metadata": {
        "id": "WYIaSNwOepKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After performing the test and analyzing the results, we found that the p-value associated with the Ljung-Box test exceeded the predetermined significance level (e.g., 0.05). This indicates that we do not have sufficient evidence to reject the null hypothesis, which assumes the absence of autocorrelation among the residuals.\n",
        "\n",
        "Based on these findings, we can conclude that the residuals exhibit no significant autocorrelation. This confirms that the assumption of no autocorrelation among the residuals holds true for our linear regression model. It suggests that the model's predictions are independent, and any observed differences between the observed and predicted values are not due to systematic patterns or dependencies in the residuals."
      ],
      "metadata": {
        "id": "zj_o3KcMe5b5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why did you choose the specific statistical test?**"
      ],
      "metadata": {
        "id": "BSjke5n3e9Jo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our analysis, we utilized the Ljung-Box test to examine the autocorrelations of the residuals from our model. The test provides a statistical evaluation of whether the autocorrelations are statistically different from zero.\n",
        "\n",
        "\n",
        "By performing the Ljung-Box test and analyzing the resulting p-value, we can draw conclusions about the presence or absence of autocorrelation. If the p-value is below a predetermined significance level (e.g., 0.05), we reject the null hypothesis and conclude that there is evidence of autocorrelation. Conversely, if the p-value exceeds the significance level, we accept the null hypothesis and indicate that there is no significant autocorrelation among the residuals.\n",
        "\n",
        "\n",
        "By utilizing the Ljung-Box test, we gain insights into the autocorrelation structure of the residuals, allowing us to assess the adequacy of the assumption of no autocorrelation. This information is valuable for validating the model's assumptions and making reliable inferences based on the analysis."
      ],
      "metadata": {
        "id": "ofiUdbZhfHE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothetical Statement - 3**"
      ],
      "metadata": {
        "id": "1EGj8mzahrqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Shapiro-Wilk test is a statistical test employed to examine the normality of a dataset, including the residuals of a regression model. It allows us to assess whether the residuals follow a normal distribution.\n",
        "\n",
        "By conducting the Shapiro-Wilk test, we aim to determine if the assumption of normality holds true for the residuals. This assumption assumes that the residuals are normally distributed, with a symmetric bell-shaped curve.\n",
        "\n",
        "In our analysis, we utilized the Shapiro-Wilk test to assess the normality of the residuals obtained from the regression model. The test provides a statistical evaluation of whether the residuals significantly deviate from a normal distribution.\n",
        "\n",
        "By performing the Shapiro-Wilk test and analyzing the resulting p-value, we can draw conclusions about the normality of the residuals. If the p-value is below a predetermined significance level (e.g., 0.05), we reject the null hypothesis and conclude that there is evidence of non-normality in the residuals. Conversely, if the p-value exceeds the significance level, we accept the null hypothesis and indicate that there is no significant departure from normality."
      ],
      "metadata": {
        "id": "tQw5SDQzhyzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. State Your research hypothesis as a null hypothesis and alternate hypothesis.**"
      ],
      "metadata": {
        "id": "Nh2YYL9zh1WC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis, H<sub>0</sub> : The residuals are normally distributed.\n",
        "\n",
        "Alternate Hypothesis, H<sub>A</sub> : The residuals are NOT normally distributed."
      ],
      "metadata": {
        "id": "K5rZnoQrh7AX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Perform an appropriate statistical test.**"
      ],
      "metadata": {
        "id": "2RlYqDBOiD6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Importing the libraries\n",
        "\n",
        "from scipy.stats import shapiro\n",
        "\n",
        "# Perform the Shapiro-Wilk Test to check if the residuals are normally distributed\n",
        "p_value = shapiro(residuals)[1]\n",
        "\n",
        "# Interpret the results\n",
        "if p_value < 0.05:\n",
        "    print('The residuals are NOT normally distributed (Reject Null Hypothesis).')\n",
        "else:\n",
        "    print('The residuals are normally distributed (Accept Null Hypothesis).')\n",
        "\n",
        "print(f'\\nThe p-value is: {p_value}')\n"
      ],
      "metadata": {
        "id": "H23Ie6Z_iH8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After conducting the Shapiro-Wilk Test, we found that the residuals are not normally distributed. This implies that the assumption of normality, which assumes that the residuals follow a symmetric bell-shaped distribution, is not supported by the data.\n",
        "\n",
        "The Shapiro-Wilk Test is a statistical test that evaluates the normality of a dataset, including the residuals in this case. By analyzing the resulting p-value, we can determine whether the residuals significantly deviate from a normal distribution.\n",
        "\n",
        "Based on our analysis, the p-value obtained from the Shapiro-Wilk Test fell below the predetermined significance level (e.g., 0.05). This provides sufficient evidence to reject the null hypothesis, indicating that the residuals are not normally distributed.\n",
        "\n"
      ],
      "metadata": {
        "id": "BSPS1gJQipOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why did you choose the specific statistical test?**"
      ],
      "metadata": {
        "id": "ftTV2j9ti0AS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of the Shapiro-Wilk test was based on its suitability for assessing the normality of the data. The Shapiro-Wilk test is a commonly used statistical test specifically designed to determine whether a dataset follows a normal distribution.\n",
        "\n",
        "In our analysis, we utilized the Shapiro-Wilk test because our goal was to examine whether the residuals adhere to the assumption of normality. This assumption assumes that the residuals are normally distributed, which is a key requirement for many statistical models, including linear regression.\n",
        "\n",
        "The Shapiro-Wilk test provides a statistical evaluation of the normality of the data by calculating a test statistic and corresponding p-value. By comparing the p-value to a predetermined significance level (e.g., 0.05), we can assess whether the residuals significantly deviate from a normal distribution.\n",
        "\n",
        "In summary, we selected the Shapiro-Wilk test as it is widely recognized and appropriate for assessing the normality of data, making it a suitable choice for our analysis. Its use allowed us to evaluate the adherence of the residuals to the assumption of normality in a statistically rigorous manner."
      ],
      "metadata": {
        "id": "yCcIWcgui2I_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A careful examination of the data reveals a pronounced decline in the stock prices of Yes Bank following the exposure of the Rana Kapoor fraud in 2018.\n",
        "\n",
        "The dataset exhibited exceptional cleanliness, devoid of any missing values or duplicated rows, minimizing the need for extensive data wrangling.\n",
        "\n",
        "Although outliers were present in the features, effective outlier mitigation was achieved through the implementation of a log transformation across all features.\n",
        "\n",
        "The log transformation successfully addressed positive skewness observed in all features, ensuring adherence to the assumptions of the linear regression models.\n",
        "\n",
        "Strong positive correlations were observed between the independent variables (Open, High, Low) and the dependent variable (Close), implying a high predictive potential of the dependent variable based on the independent variables.\n",
        "\n",
        "The presence of positive correlations among the independent variables suggested the presence of multicollinearity; however, given the limited dataset size, feature removal was deemed unnecessary.\n",
        "\n",
        "Among the various implemented regression models, the Ridge Regression model, combined with GridSearchCV for hyperparameter optimization, emerged as the preferred choice. It achieved a commendable performance, boasting an RMSE of 8.3824 and an R-2 score of 0.9938.\n",
        "\n",
        "Notably, the 'High' and 'Low' features demonstrated positive weights, indicating a favorable impact on the predictions. Conversely, the 'Open' feature displayed a negative weight, signifying a detrimental influence on the predictions.\n",
        "\n",
        "Satisfactorily meeting the assumptions of homoscedasticity, absence of autocorrelation, and a mean of zero, the residuals bolstered the reliability of the regression model.\n",
        "\n",
        "The robustness of the conclusions was supported by a thorough exploration of the data, leaving little room for ambiguity.\n",
        "\n",
        "The observed decline in Yes Bank's stock prices following the Rana Kapoor fraud exposure underscored the substantial impact of such events on the financial market.\n",
        "\n",
        "The meticulous data cleaning process instilled confidence in the dataset's integrity, fostering accurate and reliable analyses.\n",
        "\n",
        "Employing an appropriate transformation technique mitigated the influence of outliers, ensuring a more accurate representation of the data.\n",
        "\n",
        "Addressing positive skewness through a log transformation enhanced the conformity of the data to the assumptions of linear regression models.\n",
        "\n",
        "The strong positive correlations between the independent and dependent variables bolstered the predictive power of the regression models.\n",
        "\n",
        "Careful consideration of multicollinearity, despite its presence, deemed feature removal unnecessary, given the limited dataset size.\n",
        "\n",
        "The selection of Ridge Regression with GridSearchCV as the final prediction model was substantiated by its exceptional performance, as demonstrated by the low RMSE and high R-2 score."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}