# Yes-Bank-Stock-Closing-Price-Prediction
Project Summary:- This project focuses on a critical challenge: predicting the closing price of Yes Bank's stock. This prediction is crucial for stakeholders, investors, and market participants due to the bank's recent difficulties, like bad loans and fraud cases. Regulatory intervention by the Reserve Bank of India has added complexity to predicting the bank's stock prices. To address this, the project uses a comprehensive dataset covering Yes Bank's stock prices since its inception. This dataset includes key metrics like monthly closing, starting, highest, and lowest prices. The goal is to create predictive models that can grasp the complex trends in the bank's stock prices, considering its turbulent history. The project employs various modeling techniques, including time series models and regression methods. These techniques aim to accurately forecast the closing price of Yes Bank's stock and account for significant events like fraud cases and regulatory interventions. By achieving accurate predictions, this project provides valuable insights for stakeholders' investment decisions. It navigates the complexities and uncertainties surrounding Yes Bank's stock prices, ultimately aiding in better decision-making and understanding the bank's financial performance. Problem Statement:- This project centers on developing a robust predictive model for forecasting the closing price of Yes Bank's stock. The primary challenge involves comprehending the intricate dynamics of stock price trends, particularly the abrupt decline post-2018 after a period of growth. A significant hurdle lies in managing multicollinearity within the dataset, arising from interrelated independent variables. Tackling this issue is crucial to ensure the model's accuracy in prediction by adequately considering each variable's contribution. In addition, the model should be adept at accounting for major events that have left an impact on Yes Bank's stock performance. Notable occurrences, such as the involvement of the bank's founders in fraud cases and regulatory interventions by the Reserve Bank of India, have the potential to sway stock prices significantly. A key objective is for the predictive model to adeptly capture and reflect the influence of such events with precision. Furthermore, the project sets out to attain a level of predictive accuracy akin to the benchmark set by the K-Nearest Neighbors (KNN) Regression model, which achieved an impressive 99% accuracy. This high accuracy level is crucial for providing valuable insights to stakeholders, investors, and market participants. Armed with a reliable predictive tool, they can make well-informed decisions regarding investments in Yes Bank's stock, even amidst the intricate challenges posed by its price fluctuations. Ultimately, this endeavor strives to empower stakeholders with a dependable means of navigating the complexities of Yes Bank's stock performance landscape. Variables Description:- The dataset consists of monthly observations of Yes Bank stock prices since its listing on the stock exchange. The dataset includes the following features: Date: This indicates the specific month for which the stock price is recorded. Open: This represents the stock price at the beginning of the trading day when the stock exchange opens. High: This indicates the highest price reached by the stock during the given month. Low: This indicates the lowest price reached by the stock during the given month Close: This represents the stock price at the end of the trading day when the stock exchange closes. The dataset provides a comprehensive overview of the monthly performance of Yes Bank stock, including the opening, highest, lowest, and closing prices for each month since its listing on the stock exchange. Which Evaluation metrics did you consider for a positive business impact and why? In this case, the evaluation and comparison of the models' performance primarily focus on two key metrics: Root Mean Square Error (RMSE) and R-2 Score. The RMSE is a measure of the average magnitude of the prediction errors, providing insights into the models' ability to accurately estimate the Close prices. A lower RMSE indicates better predictive accuracy, as it signifies that the models' predictions are closer to the actual Close prices. The R-2 Score, also known as the coefficient of determination, quantifies the proportion of the variance in the target variable (Close prices) that is explained by the predictor variables. A higher R-2 Score indicates a better fit of the model to the data, as it suggests that a larger portion of the variation in the Close prices can be accounted for by the predictors. In this analysis, the dataset has been preprocessed to effectively handle outliers, ensuring that they do not significantly impact the models' performance. Therefore, there is no need to be concerned about the models' sensitivity to outliers. Additionally, given the small size of the dataset and the models being trained using the same predictor variables, there is no requirement to consider adjusted scores. Adjusted scores are typically used when comparing models with different sets of predictors or when dealing with larger datasets. In this case, since the models are trained on the same predictors and the dataset size is relatively small, the adjusted scores are not necessary for a meaningful comparison. By placing emphasis on RMSE and R-2 Score, we can effectively evaluate the models' predictive power and their ability to explain the variation in the Close prices. This approach allows us to determine the model As discussed earlier, Ridge Regression is a regularization technique used in Linear Regression models to address overfitting and multicollinearity. It achieves this by adding a penalty term, which is the sum performs the best in terms of accuracy and fit, ultimately contributing positively to the business objectives." Explain the model that you have used and the feature importance using any model explainability tool. As discussed earlier, Ridge Regression is a regularization technique used in Linear Regression models to address overfitting and multicollinearity. It achieves this by adding a penalty term, which is the sum of the squared values of the coefficients, to the loss function. This penalty term limits the magnitude of the coefficients, promoting a balance between the model's complexity and its ability to generalize well. On the other hand, GridSearchCV is a hyperparameter tuning technique that performs an exhaustive search over a predefined set of potential hyperparameters. It aims to find the best combination of hyperparameters for a model by evaluating them using cross-validation. When Ridge Regression is combined with GridSearchCV, Ridge Regression serves as the base model, and GridSearchCV is utilized to determine the optimal value for the regularization strength, also known as the hyperparameter controlling the penalty term. By leveraging GridSearchCV, we can systematically explore different values of the regularization strength and identify the one that yields the best performance according to the chosen evaluation metric. This approach allows us to fine-tune the Ridge Regression model and optimize its performance based on the given hyperparameters. To assess the feature importance of the Ridge Regression model, we can examine the coefficients associated with each feature. The magnitude of these coefficients indicates the relative influence of the corresponding features on the predicted outcome. By analyzing the feature importance, we gain insights into which features have the greatest impact on the predictions and can focus on interpreting their influence on the target variable. Conclusion:- A careful examination of the data reveals a pronounced decline in the stock prices of Yes Bank following the exposure of the Rana Kapoor fraud in 2018. The dataset exhibited exceptional cleanliness, devoid of any missing values or duplicated rows, minimizing the need for extensive data wrangling. Although outliers were present in the features, effective outlier mitigation was achieved through the implementation of a log transformation across all features. The log transformation successfully addressed the positive skewness observed in all features, ensuring adherence to the assumptions of the linear regression models. Strong positive correlations were observed between the independent variables (Open, High, Low) and the dependent variable (Close), implying a high predictive potential of the dependent variable based on the independent variables. The presence of positive correlations among the independent variables suggested the presence of multicollinearity; however, given the limited dataset size, feature removal was deemed unnecessary. Among the various implemented regression models, the Ridge Regression model, combined with GridSearchCV for hyperparameter optimization, emerged as the preferred choice. It achieved a commendable performance, boasting an RMSE of 8.3824 and an R-2 score of 0.9938. Notably, the 'High' and 'Low' features demonstrated positive weights, indicating a favorable impact on the predictions. Conversely, the 'Open' feature displayed a negative weight, signifying a detrimental influence on the predictions. Satisfactorily meeting the assumptions of homoscedasticity, absence of autocorrelation, and a mean of zero, the residuals bolstered the reliability of the regression model. The robustness of the conclusions was supported by a thorough exploration of the data, leaving little room for ambiguity. The observed decline in Yes Bank's stock prices following the Rana Kapoor fraud exposure underscored the substantial impact of such events on the financial market. The meticulous data-cleaning process instilled confidence in the dataset's integrity, fostering accurate and reliable analyses. Employing an appropriate transformation technique mitigated the influence of outliers, ensuring a more accurate representation of the data. Addressing positive skewness through a log transformation enhanced the conformity of the data to the assumptions of linear regression models. The strong positive correlations between the independent and dependent variables bolstered the predictive power of the regression models. Careful consideration of multicollinearity, despite its presence, deemed feature removal unnecessary, given the limited dataset size. The selection of Ridge Regression with GridSearchCV as the final prediction model was substantiated by its exceptional performance, as demonstrated by the low RMSE and high R-2 score.
